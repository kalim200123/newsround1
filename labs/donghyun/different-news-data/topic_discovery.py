import json
import re
import time
from collections import Counter
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple
import feedparser
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm
from konlpy.tag import Okt

# --- ì„¤ì • ---
MODEL_NAME = "intfloat/multilingual-e5-base"
MIN_CLUSTER_SIZE = 10
N_INITIAL_CANDIDATES = 20  # ì¤‘ë³µ ì œê±°ë¥¼ ìˆ˜í–‰í•  ì´ˆê¸° í›„ë³´ í† í”½ ìˆ˜
MAX_FINAL_TOPICS = 7       # ìµœì¢…ì ìœ¼ë¡œ ì„ íƒí•  ìµœëŒ€ í† í”½ ìˆ˜
DEDUPLICATION_THRESHOLD = 0.90 # í† í”½ì„ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨í•  ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì„ê³„ê°’

TOPIC_WORD_BLACKLIST = { "ì •ë¶€", "ëŒ€í†µë ¹ì‹¤", "ë¯¼ì£¼ë‹¹", "êµ­ë¯¼ì˜í˜", "êµ­íšŒ", "ì—¬ì•¼", "ì˜í˜¹",
                         "ë…¼ë€", "ê´€ê³„ì", "ë°œí‘œ", "ì‚¬ì§„", "ì†ë³´", "ë‹¨ë…", "ì˜¤ëŠ˜", "ë‚´ì¼", 
                         "ê´€ë ¨", "ì´ìŠˆ", "ì‚¬ì‹¤", "ìƒê°", "í•œêµ­", "ìš°ë¦¬", "ì¤‘ì•™", "ì¼ë³´", 
                         "ë‰´ìŠ¤", "ê¸°ì‚¬", "ì–¸ë¡ ", "ë³´ë„", "ê¸°ì", "ì·¨ì¬", "ì „ë¬¸ê°€", "ì „ë¬¸ê°€ë“¤",
                         "í•˜ì´ë¼ì´íŠ¸", "ì •ì¹˜", "ê²½ì œ", "ì‚¬íšŒ", "ë¬¸í™”", "ìš´ì„¸"}
# --- RSS í”¼ë“œ ì •ë³´ ---
Side = str
@dataclass
class Feed:
    source: str; source_domain: str; side: Side; url: str; section: str
FEEDS: List[Feed] = [
    Feed("ê²½í–¥ì‹ ë¬¸", "khan.co.kr", "LEFT", "https://www.khan.co.kr/rss/rssdata/politic_news.xml", "ì •ì¹˜"),
    Feed("ê²½í–¥ì‹ ë¬¸", "khan.co.kr", "LEFT", "https://www.khan.co.kr/rss/rssdata/economy_news.xml", "ê²½ì œ"),
    Feed("ê²½í–¥ì‹ ë¬¸", "khan.co.kr", "LEFT", "https://www.khan.co.kr/rss/rssdata/society_news.xml", "ì‚¬íšŒ"),
    Feed("ê²½í–¥ì‹ ë¬¸", "khan.co.kr", "LEFT", "https://www.khan.co.kr/rss/rssdata/culture_news.xml", "ë¬¸í™”"),
    Feed("í•œê²¨ë ˆ", "hani.co.kr", "LEFT", "https://www.hani.co.kr/rss/politics/", "ì •ì¹˜"),
    Feed("í•œê²¨ë ˆ", "hani.co.kr", "LEFT", "https://www.hani.co.kr/rss/economy/", "ê²½ì œ"),
    Feed("í•œê²¨ë ˆ", "hani.co.kr", "LEFT", "https://www.hani.co.kr/rss/society/", "ì‚¬íšŒ"),
    Feed("í•œê²¨ë ˆ", "hani.co.kr", "LEFT", "https://www.hani.co.kr/rss/culture/", "ë¬¸í™”"),
    Feed("ì˜¤ë§ˆì´ë‰´ìŠ¤", "ohmynews.com", "LEFT", "http://rss.ohmynews.com/rss/politics.xml", "ì •ì¹˜"),
    Feed("ì˜¤ë§ˆì´ë‰´ìŠ¤", "ohmynews.com", "LEFT", "http://rss.ohmynews.com/rss/economy.xml", "ê²½ì œ"),
    Feed("ì˜¤ë§ˆì´ë‰´ìŠ¤", "ohmynews.com", "LEFT", "http://rss.ohmynews.com/rss/society.xml", "ì‚¬íšŒ"),
    Feed("ì˜¤ë§ˆì´ë‰´ìŠ¤", "ohmynews.com", "LEFT", "http://rss.ohmynews.com/rss/culture.xml", "ë¬¸í™”"),
    Feed("ì¡°ì„ ì¼ë³´", "chosun.com", "RIGHT", "https://www.chosun.com/arc/outboundfeeds/rss/category/politics/?outputType=xml", "ì •ì¹˜"),
    Feed("ì¡°ì„ ì¼ë³´", "chosun.com", "RIGHT", "https://www.chosun.com/arc/outboundfeeds/rss/category/economy/?outputType=xml", "ê²½ì œ"),
    Feed("ì¡°ì„ ì¼ë³´", "chosun.com", "RIGHT", "https://www.chosun.com/arc/outboundfeeds/rss/category/society/?outputType=xml", "ì‚¬íšŒ"),
    Feed("ì¡°ì„ ì¼ë³´", "chosun.com", "RIGHT", "https://www.chosun.com/arc/outboundfeeds/rss/category/culture/?outputType=xml", "ë¬¸í™”"),
    Feed("ì¤‘ì•™ì¼ë³´", "joongang.co.kr", "RIGHT", "https://news.google.com/rss/search?q=site:joongang.co.kr%20ì •ì¹˜&hl=ko&gl=KR&ceid=KR%3Ako", "ì •ì¹˜"),
    Feed("ì¤‘ì•™ì¼ë³´", "joongang.co.kr", "RIGHT", "https://news.google.com/rss/search?q=site:joongang.co.kr%20ê²½ì œ&hl=ko&gl=KR&ceid=KR%3Ako", "ê²½ì œ"),
    Feed("ì¤‘ì•™ì¼ë³´", "joongang.co.kr", "RIGHT", "https://news.google.com/rss/search?q=site:joongang.co.kr%20ì‚¬íšŒ&hl=ko&gl=KR&ceid=KR%3Ako", "ì‚¬íšŒ"),
    Feed("ì¤‘ì•™ì¼ë³´", "joongang.co.kr", "RIGHT", "https://news.google.com/rss/search?q=site:joongang.co.kr%20ë¬¸í™”&hl=ko&gl=KR&ceid=KR%3Ako", "ë¬¸í™”"),
    Feed("ë™ì•„ì¼ë³´", "donga.com", "RIGHT", "https://rss.donga.com/politics.xml", "ì •ì¹˜"),
    Feed("ë™ì•„ì¼ë³´", "donga.com", "RIGHT", "https://rss.donga.com/economy.xml", "ê²½ì œ"),
    Feed("ë™ì•„ì¼ë³´", "donga.com", "RIGHT", "https://rss.donga.com/national.xml", "ì‚¬íšŒ"),
    Feed("ë™ì•„ì¼ë³´", "donga.com", "RIGHT", "https://rss.donga.com/culture.xml", "ë¬¸í™”"),
]
@dataclass
class Article:
    source: str; source_domain: str; side: Side; title: str; url: str
    published_at: Optional[str]; section: str; rss_desc: Optional[str] = None
# ... (pull_feeds, get_model, embed_texts, get_okt, extract_topic_keyword_and_desc í•¨ìˆ˜ëŠ” ë³€ê²½ ì—†ìŒ)
def pull_feeds() -> List[Article]:
    all_articles: List[Article] = []
    unique_titles = set()
    for f in tqdm(FEEDS, desc="ğŸ“° RSS í”¼ë“œ ìˆ˜ì§‘ ì¤‘"):
        feed = feedparser.parse(f.url)
        for entry in feed.entries:
            title = (getattr(entry, "title", "") or "").strip()
            if not title or title in unique_titles: continue
            unique_titles.add(title)
            link = (getattr(entry, "link", None) or "").strip()
            published = getattr(entry, "published", None) or getattr(entry, "updated", None)
            desc = re.sub(r"<[^>]+>", " ", getattr(entry, "summary", "")).strip()
            all_articles.append(Article(source=f.source, source_domain=f.source_domain, side=f.side, title=title, url=link, published_at=published, section=f.section, rss_desc=desc))
        time.sleep(0.05)
    return all_articles
_MODEL = None
def get_model():
    global _MODEL
    if _MODEL is None: _MODEL = SentenceTransformer(MODEL_NAME)
    return _MODEL
def embed_texts(texts: List[str]) -> np.ndarray:
    model = get_model()
    prefixed_texts = [f"passage: {text[:512]}" for text in texts]
    vecs = model.encode(prefixed_texts, batch_size=128, show_progress_bar=True, normalize_embeddings=True)
    return np.asarray(vecs, dtype=np.float32)
_OKT = None
def get_okt():
    global _OKT
    if _OKT is None: _OKT = Okt()
    return _OKT
def extract_topic_keyword_and_desc(titles: List[str]) -> Optional[Tuple[str, str]]:
    okt = get_okt()
    full_text = " ".join(titles)
    nouns = okt.nouns(full_text)
    filtered_nouns = [n for n in nouns if len(n) > 1 and n.lower() not in TOPIC_WORD_BLACKLIST]
    if not filtered_nouns: return None
    counts = Counter(filtered_nouns)
    most_common = counts.most_common(4)
    if not most_common: return None
    core_keyword = most_common[0][0]
    sub_desc_keywords = [kw for kw, count in most_common[1:4]]
    sub_description = ", ".join(sub_desc_keywords)
    return core_keyword, sub_description

def deduplicate_topics(topic_candidates: List[Dict]) -> List[Dict]:
    """
    ì£¼ì–´ì§„ í† í”½ í›„ë³´ ëª©ë¡ì—ì„œ ì¤‘ë³µëœ í† í”½ì„ ì œê±°í•©ë‹ˆë‹¤.
    - ê° í† í”½ì˜ í‚¤ì›Œë“œë¥¼ ì„ë² ë”©í•˜ì—¬ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    - ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ì„ê³„ê°’ ì´ìƒì¸ ê²½ìš° ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.
    - ì¤‘ë³µëœ í† í”½ ì¤‘ì—ì„œëŠ” ì ìˆ˜(_score)ê°€ ê°€ì¥ ë†’ì€ í† í”½ í•˜ë‚˜ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
    """
    if len(topic_candidates) < 2:
        return topic_candidates

    # ê° í† í”½ì˜ ëŒ€í‘œ í…ìŠ¤íŠ¸ ìƒì„± (í‚¤ì›Œë“œ ì¡°í•©)
    topic_texts = [f"{t['core_keyword']} {t['sub_description']}" for t in topic_candidates]
    
    # ëŒ€í‘œ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©
    print("- í† í”½ í‚¤ì›Œë“œ ì„ë² ë”© ì¤‘...")
    topic_embeds = embed_texts(topic_texts)

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    print("- ìœ ì‚¬ë„ ê³„ì‚° ë° ì¤‘ë³µ ì œê±° ì¤‘...")
    similarity_matrix = cosine_similarity(topic_embeds)

    # ì¤‘ë³µë˜ì§€ ì•Šì€ í† í”½ë§Œ ë‚¨ê¸°ê¸°
    unique_candidates = []
    is_duplicate = [False] * len(topic_candidates)

    # ì´ë¯¸ ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, ì•ìª½ í† í”½ì´ í•­ìƒ ì ìˆ˜ê°€ ë†’ê±°ë‚˜ ê°™ìŒ
    for i in range(len(topic_candidates)):
        if is_duplicate[i]:
            continue
        # ìê¸° ìì‹ ì€ ìœ ë‹ˆí¬ í›„ë³´ì— ì¶”ê°€
        unique_candidates.append(topic_candidates[i])
        # ë‚˜ë¨¸ì§€ í† í”½ë“¤ê³¼ ìœ ì‚¬ë„ ë¹„êµ
        for j in range(i + 1, len(topic_candidates)):
            if not is_duplicate[j] and similarity_matrix[i][j] > DEDUPLICATION_THRESHOLD:
                # ì„ê³„ê°’ì„ ë„˜ìœ¼ë©´ ì¤‘ë³µìœ¼ë¡œ ì²˜ë¦¬ (ì ìˆ˜ê°€ ë‚®ì€ ìª½)
                is_duplicate[j] = True
                print(f"  - ì¤‘ë³µ ë°œê²¬: '{topic_candidates[i]['core_keyword']}' > '{topic_candidates[j]['core_keyword']}' (ìœ ì‚¬ë„: {similarity_matrix[i][j]:.2f}) - ì ìˆ˜ê°€ ë‚®ì€ í† í”½ì„ ì œê±°í•©ë‹ˆë‹¤.")

    return unique_candidates

def main():
    # 1. RSS ìˆ˜ì§‘ ë° ê°€ì¤‘ì¹˜ ì ìš© ì„ë² ë”©
    articles = pull_feeds()
    if len(articles) < 50:
        print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ë„ˆë¬´ ì ì–´ ë¶„ì„ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    titles = [a.title for a in articles]
    descs  = [a.rss_desc or "" for a in articles]
    title_embeds = embed_texts(titles)
    desc_embeds  = embed_texts(descs)
    article_embeds = 0.7 * title_embeds + 0.3 * desc_embeds
    article_embeds = article_embeds / (np.linalg.norm(article_embeds, axis=1, keepdims=True) + 1e-8)

    # 2. êµ°ì§‘ ìƒì„±
    n_clusters = 25
    kmeans = KMeans(n_clusters=n_clusters, n_init='auto', random_state=42)
    labels = kmeans.fit_predict(article_embeds)
    clusters: Dict[int, List[int]] = {i: [] for i in range(n_clusters)}
    for i, label in enumerate(labels):
        clusters[label].append(i)

    # 3. í›„ë³´ ìƒì„± ë° ì ìˆ˜ ê³„ì‚°
    topic_candidates = []
    centroids_map = {i: np.mean(article_embeds[idxs], axis=0) for i, idxs in clusters.items() if len(idxs) > 0}

    for i, idxs in tqdm(clusters.items(), desc="ğŸ” í† í”½ í›„ë³´ ìƒì„± ë° ì ìˆ˜ ê³„ì‚° ì¤‘"):
        if len(idxs) < MIN_CLUSTER_SIZE:
            continue

        group_articles = [articles[i] for i in idxs]
        keyword_result = extract_topic_keyword_and_desc([a.title for a in group_articles])
        if not keyword_result:
            continue

        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        centroid = centroids_map[i]
        cohesion = np.mean(cosine_similarity(article_embeds[idxs], centroid.reshape(1, -1)))
        
        side_counts = Counter(a.side for a in group_articles)
        left = side_counts.get("LEFT", 0)
        right = side_counts.get("RIGHT", 0)
        balance = min(left, right) / max(left, right) if left > 0 and right > 0 else 0.0
        
        coverage = min(1.0, (len(group_articles) / 50.0) * 0.7 + (len({a.source_domain for a in group_articles}) / 6.0) * 0.3)
        
        score = (0.40 * cohesion) + (0.30 * balance) + (0.30 * coverage)

        topic_candidates.append({
            "core_keyword": keyword_result[0],
            "sub_description": keyword_result[1],
            "_score": score,
            "stats": {
                "total_articles": len(group_articles),
                "left_articles": left,
                "right_articles": right,
                "source_count": len({a.source_domain for a in group_articles}),
            },
            "sample_titles": [a.title for a in group_articles[:5]],
        })

    # 4. ì ìˆ˜ ê¸°ë°˜ ì •ë ¬ ë° ì´ˆê¸° í›„ë³´ ì„ íƒ
    topic_candidates.sort(key=lambda t: t["_score"], reverse=True)
    initial_candidates = topic_candidates[:N_INITIAL_CANDIDATES]

    # 5. ì¤‘ë³µ ì œê±°
    print(f"\nğŸ”„ {len(initial_candidates)}ê°œì˜ ì´ˆê¸° í›„ë³´ì— ëŒ€í•´ ì¤‘ë³µ ì œê±°ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    deduplicated_candidates = deduplicate_topics(initial_candidates)

    # 6. ìµœì¢… í† í”½ ì„ íƒ
    final_candidates = deduplicated_candidates[:MAX_FINAL_TOPICS]

    # 7. ìµœì¢… ê²°ê³¼ë¬¼ ì •ë¦¬
    final_results = []
    for cand in final_candidates:
        # DBì— ë„£ì„ ê¹”ë”í•œ ë°ì´í„°ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
        final_results.append({
            "core_keyword": cand["core_keyword"],
            "sub_description": cand["sub_description"],
            "stats": cand["stats"],
            "sample_titles": cand["sample_titles"],
        })

    print(f"\nâœ… ì¤‘ë³µ ì œê±° í›„ {len(final_results)}ê°œì˜ ìµœì¢… í† í”½ì„ ì„ ë°œí–ˆìŠµë‹ˆë‹¤.")
    with open("suggested_topics.json", "w", encoding="utf-8") as f:
        json.dump(final_results, f, ensure_ascii=False, indent=2)
    print("\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ! 'suggested_topics.json' íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == '__main__':
    main()